<!DOCTYPE html>
<html lang="en">

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
  <title>pacman | Generalist</title>
  <link rel="icon" type="image/png" sizes="32x32" href="[object]">
  <link rel="stylesheet" href="https://eiasamoah.me/styles/styles.css" />
</head>

<body class="min-h-screen bg-background text-foreground" style="font-family: Helvetica, sans-serif;">
  <div class="Root">

    <header class="sticky top-0 z-50 w-full border-b border-(--tertiary-color) backdrop-blur-sm">
      <div class="container max-w-7xl mx-auto">
        <nav class="flex h-16 items-center justify-between">
          <a class="flex items-center space-x-2 text-xl" href="https://eiasamoah.me/">pacman</a>
          <div class="hidden md:flex items-center space-x-8">
            <a href="https://eiasamoah.me/blog/" class="relative text-sm font-medium text-(--text-muted) transition-colors a-hover group">Blog</a>
            <a href="https://eiasamoah.me/project/" class="relative text-sm font-medium text-(--text-muted) transition-colors a-hover group">Projects</a>
            <a href="https://eiasamoah.me/research/" class="relative text-sm font-medium text-(--text-muted) transition-colors a-hover group">Research</a>
            <!-- <a href="#" class="relative text-sm font-medium text-(--text-muted) transition-colors a-hover group">Resume</a> -->
          </div>
          <div class="flex items-center space-x-8 md:hidden">
            <a href="#" class="relative text-sm font-medium text-(--text-muted) transition-colors a-hover group">Resume</a>
          </div>
        </nav>
      </div>
    </header>

    <div class="mx-auto p-10 max-w-5xl">
      

<div class="prose prose-base sm:prose-md xl:prose-lg max-w-5xl">
	<h2 align="center">Vision Transformer</h2>

<img src="https:&#x2F;&#x2F;eiasamoah.me&#x2F;processed_images&#x2F;vision_transformers.8bbbe42b3bb030ba.png" /><h3 id="composition">Composition</h3>
<p>It comprises of many layers.</p>
<ul>
<li>Layer -&gt; takes an input, performs a function on it, returns an output</li>
<li>Block -&gt; collection of layers</li>
<li>Architecture/Model -&gt; collection of blocks</li>
</ul>
<h2 id="architecture">Architecture</h2>
<ul>
<li>Embeddings -&gt; images turned into patches (learnable representations)</li>
<li>Norm -&gt; Layer normalization for regularization</li>
<li>Multi-Head Attention</li>
<li>MLP -&gt; collection of feedforward layers with an activation function <a href="https://paperswithcode.com/method/gelu">GELU</a>  and dropout</li>
<li>Transformer Encoder -&gt; two skip connections present. layer's inputs are fed to immediate layers as well as subsequent layers</li>
<li>MLP Head -&gt; output layer/ it converts learned features into a class output</li>
</ul>
<p>Math</p>
<p><img src="https://cdn.mathpix.com/snip/images/n8H8xTUOHzSF-M4TMYKVIJS4Ix9rDCqFqMR6Ettjix0.original.fullsize.png" alt="eqn" /></p>
<h2 id="architecture-flow">Architecture Flow</h2>
<h3 id="calculating-patch-embedding-input-and-output-shapes-by-hand">Calculating patch embedding input and output shapes by hand</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span>height = </span><span style="color:#d08770;">224 </span><span style="color:#65737e;"># H
</span><span>width = </span><span style="color:#d08770;">224 </span><span style="color:#65737e;"># W
</span><span>color_channels = </span><span style="color:#d08770;">3 </span><span style="color:#65737e;"># C
</span><span>patch_size = </span><span style="color:#d08770;">16 </span><span style="color:#65737e;"># P
</span><span>
</span><span>number_of_patches = (height * width) / patch_size**</span><span style="color:#d08770;">2
</span></code></pre>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span style="color:#65737e;"># size of single image
</span><span>embedding_layer_input_shape = (height, width, color_channels)
</span><span>
</span><span style="color:#65737e;">#output shape after being converted
</span><span>embedding_layer_input_shape = (number_of_patches, (patch_size**</span><span style="color:#d08770;">2 </span><span>* color_channels))
</span></code></pre>
<h3 id="turning-a-single-image-into-patches-with-nn-conv2d">Turning a single image into patches with nn.Conv2d()</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span style="color:#b48ead;">from </span><span>torch </span><span style="color:#b48ead;">import </span><span>nn
</span><span>
</span><span>patch_size = </span><span style="color:#d08770;">16
</span><span>
</span><span>conv2d = nn.</span><span style="color:#bf616a;">Conv2d</span><span>(</span><span style="color:#bf616a;">in_channels</span><span>=</span><span style="color:#d08770;">3</span><span>,
</span><span>                   </span><span style="color:#bf616a;">out_channels</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#65737e;"># From Table 1: This is the hidden size
</span><span>                   </span><span style="color:#bf616a;">kernel_size</span><span>=patch_size,
</span><span>                   </span><span style="color:#bf616a;">stride</span><span>=patch_size,
</span><span>                   </span><span style="color:#bf616a;">padding</span><span>=</span><span style="color:#d08770;">0
</span><span>                   )
</span><span style="color:#65737e;"># (embedding_dim, height, width) -&gt; [768, 14, 14]
</span><span style="color:#65737e;"># unsqueeze(0) -&gt; (batch, embedding_dim, feature_map_height, feature_map_width) -&gt; [1, 768, 14, 14]
</span></code></pre>
<h3 id="flattening-the-patch-embedding-with-torch-nn-flatten">Flattening the patch embedding with torch.nn.Flatten()</h3>
<p>The spatial dimensions are what needs to be flattened eg. <code>feature_map_height</code> and <code>feature_map_width</code> which are at pos 2 and 3 in the output</p>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span>flatten = nn.</span><span style="color:#bf616a;">Flatten</span><span>(</span><span style="color:#bf616a;">start_dim</span><span>=</span><span style="color:#d08770;">2</span><span>, </span><span style="color:#bf616a;">end_dim</span><span>=</span><span style="color:#d08770;">3</span><span>) </span><span style="color:#65737e;"># flatten feature_map_height and feature_map_width
</span></code></pre>
<ul>
<li>current shape: (1, 768, 196)</li>
<li>desired: (1, 196, 768)</li>
</ul>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span>image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.</span><span style="color:#bf616a;">permute</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#d08770;">2</span><span>, </span><span style="color:#d08770;">1</span><span>)
</span><span>
</span><span style="color:#65737e;"># [batch_size, P^2*C, N] -&gt; [batch_size, N, P^2*C]
</span><span>
</span></code></pre>
<blockquote>
<p>The original Transformer architecture was designed to work with text.
The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images.
This is why the input to the ViT architecture is processed in the way it is.
We're essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p>
</blockquote>
<h3 id="turning-the-vit-patch-embedding-layer-into-a-pytorch-module">Turning the ViT patch embedding layer into a PyTorch module</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>
</span><span style="color:#b48ead;">import </span><span>torch
</span><span style="color:#b48ead;">import </span><span>torch.nn </span><span style="color:#b48ead;">as </span><span>nn
</span><span>
</span><span>
</span><span style="color:#b48ead;">class </span><span style="color:#ebcb8b;">PatchEmbedding</span><span style="color:#eff1f5;">(</span><span style="color:#a3be8c;">nn.Module</span><span style="color:#eff1f5;">):
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#96b5b4;">__init__</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">in_channels</span><span>=</span><span style="color:#d08770;">3</span><span>, </span><span style="color:#bf616a;">embedding_dim</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#bf616a;">patch_size</span><span>=</span><span style="color:#d08770;">16</span><span>):
</span><span>        </span><span style="color:#96b5b4;">super</span><span>().</span><span style="color:#96b5b4;">__init__</span><span>()
</span><span>        </span><span style="color:#bf616a;">self</span><span>.in_channels = in_channels
</span><span>        </span><span style="color:#bf616a;">self</span><span>.patch_size = patch_size
</span><span>        </span><span style="color:#bf616a;">self</span><span>.embedding_dim = embedding_dim
</span><span>        </span><span style="color:#bf616a;">self</span><span>.conv = nn.</span><span style="color:#bf616a;">Conv2d</span><span>(
</span><span>            </span><span style="color:#bf616a;">in_channels</span><span>=</span><span style="color:#bf616a;">self</span><span>.in_channels,
</span><span>            </span><span style="color:#bf616a;">out_channels</span><span>=</span><span style="color:#bf616a;">self</span><span>.embedding_dim,
</span><span>            </span><span style="color:#bf616a;">kernel_size</span><span>=</span><span style="color:#bf616a;">self</span><span>.patch_size,
</span><span>            </span><span style="color:#bf616a;">stride</span><span>=</span><span style="color:#bf616a;">self</span><span>.patch_size,
</span><span>            </span><span style="color:#bf616a;">padding</span><span>=</span><span style="color:#d08770;">0
</span><span>        )
</span><span>        </span><span style="color:#bf616a;">self</span><span>.flatten = nn.</span><span style="color:#bf616a;">Flatten</span><span>(</span><span style="color:#bf616a;">start_dim</span><span>=</span><span style="color:#d08770;">2</span><span>, </span><span style="color:#bf616a;">end_dim</span><span>=</span><span style="color:#d08770;">3</span><span>)
</span><span>
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">X</span><span>):
</span><span>        </span><span style="color:#b48ead;">assert </span><span>X.shape[-</span><span style="color:#d08770;">1</span><span>] % </span><span style="color:#bf616a;">self</span><span>.patch_size == </span><span style="color:#d08770;">0</span><span>, &quot;</span><span style="color:#a3be8c;">Input dimensions must be divisible</span><span>&quot;
</span><span>        out_conv = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">conv</span><span>(X.</span><span style="color:#bf616a;">unsqueeze</span><span>(</span><span style="color:#d08770;">0</span><span>)) </span><span style="color:#65737e;"># [1, 768, 14, 14]
</span><span>        out_conv = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">flatten</span><span>(out_conv) </span><span style="color:#65737e;"># [1, 768, 196]
</span><span>        out_conv = out_conv.</span><span style="color:#bf616a;">permute</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#d08770;">2</span><span>, </span><span style="color:#d08770;">1</span><span>) </span><span style="color:#65737e;"># [1, 196, 768]
</span><span>        </span><span style="color:#b48ead;">return </span><span>out_conv
</span><span>
</span></code></pre>
<h3 id="creating-class-token-embedding">Creating class token embedding</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#65737e;"># Get the batch size and embedding dimension
</span><span>batch_size = patch_embedded_image.shape[</span><span style="color:#d08770;">0</span><span>]
</span><span>embedding_dimension = patch_embedded_image.shape[-</span><span style="color:#d08770;">1</span><span>]
</span><span>
</span><span style="color:#65737e;"># Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)
</span><span>class_token = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">ones</span><span>(batch_size, </span><span style="color:#d08770;">1</span><span>, embedding_dimension), </span><span style="color:#65737e;"># [batch_size, number_of_tokens, embedding_dimension]
</span><span>                           </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>) </span><span style="color:#65737e;"># make sure the embedding is learnable
</span><span>
</span><span style="color:#65737e;"># Print the class_token shape
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Class token shape: </span><span>{class_token.shape}</span><span style="color:#a3be8c;"> -&gt; [batch_size, number_of_tokens, embedding_dimension]</span><span>&quot;)
</span></code></pre>
<ul>
<li>We prepend the class token to the patched images</li>
</ul>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>patch_embedded_image_with_class_embedding = torch.</span><span style="color:#bf616a;">cat</span><span>((class_token, patched_embedded_image), </span><span style="color:#bf616a;">dim</span><span>=</span><span style="color:#d08770;">1</span><span>) </span><span style="color:#65737e;"># concat on the first dimension
</span></code></pre>
<h3 id="creating-the-position-embedding">Creating the position embedding</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#65737e;"># Calculate N (number of patches)
</span><span>number_of_patches = </span><span style="color:#bf616a;">int</span><span>((height * width) / patch_size**</span><span style="color:#d08770;">2</span><span>)
</span><span>
</span><span style="color:#65737e;"># Get embedding dimension
</span><span>embedding_dimension = patch_embedded_image_with_class_embedding.shape[</span><span style="color:#d08770;">2</span><span>]
</span><span>
</span><span style="color:#65737e;"># Create the learnable 1D position embedding
</span><span>position_embedding = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">ones</span><span>(</span><span style="color:#d08770;">1</span><span>,
</span><span>                                             number_of_patches+</span><span style="color:#d08770;">1</span><span>,
</span><span>                                             embedding_dimension),
</span><span>                                  </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>) </span><span style="color:#65737e;"># make sure it&#39;s learnable
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Position embedding shape: </span><span>{position_embedding.shape}</span><span style="color:#a3be8c;"> -&gt; [batch_size, number_of_patches, embedding_dimension]</span><span>&quot;)
</span></code></pre>
<ul>
<li>Add the position embedding to the patch and class token embedding</li>
</ul>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding
</span></code></pre>
<h3 id="putting-it-all-together-from-image-to-embedding">Putting it all together: from image to embedding</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#bf616a;">set_seeds</span><span>()
</span><span>
</span><span style="color:#65737e;"># 1. Set patch size
</span><span>patch_size = </span><span style="color:#d08770;">16
</span><span>
</span><span style="color:#65737e;"># 2. Print shape of original image tensor and get the image dimensions
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Image tensor shape: </span><span>{image.shape}&quot;)
</span><span>height, width = image.shape[</span><span style="color:#d08770;">1</span><span>], image.shape[</span><span style="color:#d08770;">2</span><span>]
</span><span>
</span><span style="color:#65737e;"># 3. Get image tensor and add batch dimension
</span><span>x = image.</span><span style="color:#bf616a;">unsqueeze</span><span>(</span><span style="color:#d08770;">0</span><span>)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Input image with batch dimension shape: </span><span>{x.shape}&quot;)
</span><span>
</span><span style="color:#65737e;"># 4. Create patch embedding layer
</span><span>patch_embedding_layer = </span><span style="color:#bf616a;">PatchEmbedding</span><span>(</span><span style="color:#bf616a;">in_channels</span><span>=</span><span style="color:#d08770;">3</span><span>,
</span><span>                                       </span><span style="color:#bf616a;">patch_size</span><span>=patch_size,
</span><span>                                       </span><span style="color:#bf616a;">embedding_dim</span><span>=</span><span style="color:#d08770;">768</span><span>)
</span><span>
</span><span style="color:#65737e;"># 5. Pass image through patch embedding layer
</span><span>patch_embedding = </span><span style="color:#bf616a;">patch_embedding_layer</span><span>(x)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Patching embedding shape: </span><span>{patch_embedding.shape}&quot;)
</span><span>
</span><span style="color:#65737e;"># 6. Create class token embedding
</span><span>batch_size = patch_embedding.shape[</span><span style="color:#d08770;">0</span><span>]
</span><span>embedding_dimension = patch_embedding.shape[-</span><span style="color:#d08770;">1</span><span>]
</span><span>class_token = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">ones</span><span>(batch_size, </span><span style="color:#d08770;">1</span><span>, embedding_dimension),
</span><span>                           </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>) </span><span style="color:#65737e;"># make sure it&#39;s learnable
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Class token embedding shape: </span><span>{class_token.shape}&quot;)
</span><span>
</span><span style="color:#65737e;"># 7. Prepend class token embedding to patch embedding
</span><span>patch_embedding_class_token = torch.</span><span style="color:#bf616a;">cat</span><span>((class_token, patch_embedding), </span><span style="color:#bf616a;">dim</span><span>=</span><span style="color:#d08770;">1</span><span>)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Patch embedding with class token shape: </span><span>{patch_embedding_class_token.shape}&quot;)
</span><span>
</span><span style="color:#65737e;"># 8. Create position embedding
</span><span>number_of_patches = </span><span style="color:#bf616a;">int</span><span>((height * width) / patch_size**</span><span style="color:#d08770;">2</span><span>)
</span><span>position_embedding = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">ones</span><span>(</span><span style="color:#d08770;">1</span><span>, number_of_patches+</span><span style="color:#d08770;">1</span><span>, embedding_dimension),
</span><span>                                  </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>) </span><span style="color:#65737e;"># make sure it&#39;s learnable
</span><span>
</span><span style="color:#65737e;"># 9. Add position embedding to patch embedding with class token
</span><span>patch_and_position_embedding = patch_embedding_class_token + position_embedding
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Patch and position embedding shape: </span><span>{patch_and_position_embedding.shape}&quot;)
</span></code></pre>
<h3 id="replicating-equation-2-with-pytorch-layers">Replicating Equation 2 with Pytorch Layers</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#b48ead;">import </span><span>torch
</span><span style="color:#b48ead;">import </span><span>torch.nn </span><span style="color:#b48ead;">as </span><span>nn
</span><span>
</span><span style="color:#b48ead;">class </span><span style="color:#ebcb8b;">MultiheadSelfAttentionBlock</span><span style="color:#eff1f5;">(</span><span style="color:#a3be8c;">nn.Module</span><span style="color:#eff1f5;">):
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#96b5b4;">__init__</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">embed_dim</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#bf616a;">num_heads</span><span>=</span><span style="color:#d08770;">12</span><span>, </span><span style="color:#bf616a;">dropout</span><span>=</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">batch_first</span><span>=</span><span style="color:#d08770;">True</span><span>):
</span><span>        </span><span style="color:#96b5b4;">super</span><span>().</span><span style="color:#96b5b4;">__init__</span><span>()
</span><span>        </span><span style="color:#bf616a;">self</span><span>.embed_dim = embed_dim
</span><span>        </span><span style="color:#bf616a;">self</span><span>.num_heads = num_heads
</span><span>        </span><span style="color:#bf616a;">self</span><span>.layer = nn.</span><span style="color:#bf616a;">LayerNorm</span><span>(normalized_shapeself.embed_dim)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.attention = nn.</span><span style="color:#bf616a;">MultiheadAttention</span><span>(embed_dim, num_heads, </span><span style="color:#bf616a;">dropout</span><span>=dropout, </span><span style="color:#bf616a;">batch_first</span><span>=batch_first)
</span><span>
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">X</span><span>):
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">layer</span><span>(X)
</span><span>        X, </span><span style="color:#bf616a;">_ </span><span>= </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">attention</span><span>(
</span><span>            </span><span style="color:#bf616a;">query</span><span>=X,
</span><span>            </span><span style="color:#bf616a;">key</span><span>=X,
</span><span>            </span><span style="color:#bf616a;">value</span><span>=X,
</span><span>            </span><span style="color:#bf616a;">need_weights</span><span>=</span><span style="color:#d08770;">False
</span><span>        )
</span><span>        </span><span style="color:#b48ead;">return </span><span>X
</span></code></pre>
<ul>
<li>An instance is created and the <code>patch_and_position_embedding</code> is passed in</li>
</ul>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>multihead_self_attention_block = </span><span style="color:#bf616a;">MultiheadSelfAttentionBlock</span><span>(embedding_dim, </span><span style="color:#bf616a;">num_heads</span><span>=</span><span style="color:#d08770;">12</span><span>)
</span><span>
</span><span style="color:#bf616a;">multihead_sef_attention_block</span><span>(patch_and_position_embedding)
</span><span>
</span></code></pre>
<h3 id="multilayer-peceptron">MultiLayer Peceptron</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#b48ead;">class </span><span style="color:#ebcb8b;">MLPBlock</span><span style="color:#eff1f5;">(</span><span style="color:#a3be8c;">nn.Module</span><span style="color:#eff1f5;">):
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#96b5b4;">__init__</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">embed_dim</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#bf616a;">mlp_size</span><span>=</span><span style="color:#d08770;">3072</span><span>, </span><span style="color:#bf616a;">dropout</span><span>=</span><span style="color:#d08770;">0.1</span><span>):
</span><span>        </span><span style="color:#96b5b4;">super</span><span>().</span><span style="color:#96b5b4;">__init__</span><span>()
</span><span>        </span><span style="color:#bf616a;">self</span><span>.embed_dim = embed_dim
</span><span>        </span><span style="color:#bf616a;">self</span><span>.dropout = nn.</span><span style="color:#bf616a;">Dropout</span><span>(</span><span style="color:#bf616a;">p</span><span>=dropout)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.linear1 = nn.</span><span style="color:#bf616a;">Linear</span><span>(</span><span style="color:#bf616a;">in_features</span><span>=embed_dim, </span><span style="color:#bf616a;">out_features</span><span>=mlp_size)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.linear2 = nn.</span><span style="color:#bf616a;">Linear</span><span>(</span><span style="color:#bf616a;">in_features</span><span>=mlp_size, </span><span style="color:#bf616a;">out_features</span><span>=embed_dim)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.gelu = nn.</span><span style="color:#bf616a;">GELU</span><span>()
</span><span>        </span><span style="color:#bf616a;">self</span><span>.norm = nn.</span><span style="color:#bf616a;">LayerNorm</span><span>(</span><span style="color:#bf616a;">normalized_shape</span><span>=embed_dim)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.seq = nn.</span><span style="color:#bf616a;">Sequential</span><span>(
</span><span>            </span><span style="color:#bf616a;">self</span><span>.linear1,
</span><span>            </span><span style="color:#bf616a;">self</span><span>.gelu,
</span><span>            </span><span style="color:#bf616a;">self</span><span>.dropout,
</span><span>            </span><span style="color:#bf616a;">self</span><span>.linear2,
</span><span>            </span><span style="color:#bf616a;">self</span><span>.dropout
</span><span>        )
</span><span>
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">X</span><span>):
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">norm</span><span>(X)
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">seq</span><span>(X)
</span><span>        </span><span style="color:#b48ead;">return </span><span>X
</span></code></pre>
<ul>
<li>Instance</li>
</ul>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#65737e;"># Create an instance of MLPBlock
</span><span>mlp_block = </span><span style="color:#bf616a;">MLPBlock</span><span>(</span><span style="color:#bf616a;">embedding_dim</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#65737e;"># from Table 1
</span><span>                     </span><span style="color:#bf616a;">mlp_size</span><span>=</span><span style="color:#d08770;">3072</span><span>, </span><span style="color:#65737e;"># from Table 1
</span><span>                     </span><span style="color:#bf616a;">dropout</span><span>=</span><span style="color:#d08770;">0.1</span><span>) </span><span style="color:#65737e;"># from Table 3
</span><span>
</span><span style="color:#65737e;"># Pass output of MSABlock through MLPBlock
</span><span>patched_image_through_mlp_block = </span><span style="color:#bf616a;">mlp_block</span><span>(patched_image_through_msa_block)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Input shape of MLP block: </span><span>{patched_image_through_msa_block.shape}&quot;)
</span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Output shape MLP block: </span><span>{patched_image_through_mlp_block.shape}&quot;)
</span></code></pre>
<h3 id="create-the-transformer-encoder">Create the Transformer Encoder</h3>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#b48ead;">class </span><span style="color:#ebcb8b;">TransformerEncoderBlock</span><span style="color:#eff1f5;">(</span><span style="color:#a3be8c;">nn.Module</span><span style="color:#eff1f5;">):
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#96b5b4;">__init__</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">embed_dim</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#bf616a;">num_heads</span><span>=</span><span style="color:#d08770;">12</span><span>, </span><span style="color:#bf616a;">att_dropout</span><span>=</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">batch_first</span><span>=</span><span style="color:#d08770;">True</span><span>, </span><span style="color:#bf616a;">mlp_size</span><span>=</span><span style="color:#d08770;">3072</span><span>, </span><span style="color:#bf616a;">mlp_dropout</span><span>=</span><span style="color:#d08770;">0.1</span><span>, </span><span style="color:#bf616a;">MSA</span><span>: MutliheadSelfAttentionBlock, </span><span style="color:#bf616a;">MLP</span><span>: MLPBlock):
</span><span>        </span><span style="color:#96b5b4;">super</span><span>().</span><span style="color:#96b5b4;">__init__</span><span>()
</span><span>        </span><span style="color:#bf616a;">self</span><span>.embed_dim embed_dim
</span><span>        </span><span style="color:#bf616a;">self</span><span>.norm = nn.</span><span style="color:#bf616a;">LayerNorm</span><span>(</span><span style="color:#bf616a;">normalized_shape</span><span>=embed_dim)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.attention = </span><span style="color:#bf616a;">MSA</span><span>(embed_dim, num_heads, att_dropout, batch_first)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.mlp = </span><span style="color:#bf616a;">MLP</span><span>(embed_dim, mlp_size, mlp_dropout)
</span><span>
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">X</span><span>):
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">attention</span><span>(X) + X
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">mlp</span><span>(X) + X
</span><span>        </span><span style="color:#b48ead;">return </span><span>X
</span></code></pre>
<h2 id="putting-it-all-together-to-create-vit">Putting it all together to create ViT</h2>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span style="color:#b48ead;">class </span><span style="color:#ebcb8b;">ViT</span><span style="color:#eff1f5;">(</span><span style="color:#a3be8c;">nn.Module</span><span style="color:#eff1f5;">):
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#96b5b4;">__init__</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">img_size</span><span>=</span><span style="color:#d08770;">224</span><span>, </span><span style="color:#bf616a;">in_channels</span><span>=</span><span style="color:#d08770;">3</span><span>, </span><span style="color:#bf616a;">num_layers</span><span>=</span><span style="color:#d08770;">12</span><span>, </span><span style="color:#bf616a;">hidden_size</span><span>=</span><span style="color:#d08770;">768</span><span>, </span><span style="color:#bf616a;">mlp_size</span><span>=</span><span style="color:#d08770;">3072</span><span>, </span><span style="color:#bf616a;">num_heads</span><span>=</span><span style="color:#d08770;">12</span><span>, </span><span style="color:#bf616a;">patch_size</span><span>=</span><span style="color:#d08770;">16</span><span>, </span><span style="color:#bf616a;">att_dropout</span><span>=</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">mlp_dropout</span><span>=</span><span style="color:#d08770;">0.1</span><span>, </span><span style="color:#bf616a;">batch_first</span><span>=</span><span style="color:#d08770;">True</span><span>, </span><span style="color:#bf616a;">embedding_dropout</span><span>=</span><span style="color:#d08770;">0.1</span><span>, </span><span style="color:#bf616a;">num_classes</span><span>=</span><span style="color:#d08770;">1000</span><span>, </span><span style="color:#bf616a;">patch_embed</span><span>: PatchEmbedding, </span><span style="color:#bf616a;">msa</span><span>: MultiheadSelfAttention, </span><span style="color:#bf616a;">mlp</span><span>: MLPBlock, </span><span style="color:#bf616a;">encoder</span><span>:TransformerEncoderBlock):
</span><span>        </span><span style="color:#96b5b4;">super</span><span>().</span><span style="color:#96b5b4;">__init__</span><span>()
</span><span>
</span><span>        </span><span style="color:#b48ead;">assert </span><span style="color:#bf616a;">self</span><span>.hidden_size % </span><span style="color:#bf616a;">self</span><span>.patch_size == </span><span style="color:#d08770;">0</span><span>, &quot;</span><span style="color:#a3be8c;">Image dimensions are wrong</span><span>&quot;
</span><span>        </span><span style="color:#bf616a;">self</span><span>.num_of_patches = (img_size * img_size) / (patch_size **</span><span style="color:#d08770;">2</span><span>)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.class_embedding = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">randn</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">1</span><span>, hidden_size),
</span><span>                           </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.position_embedding = nn.</span><span style="color:#bf616a;">Parameter</span><span>(torch.</span><span style="color:#bf616a;">randn</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#bf616a;">self</span><span>.num_of_patches+</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#bf616a;">self</span><span>.hidden_size), </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.embedding_dropout = nn.</span><span style="color:#bf616a;">Dropout</span><span>(</span><span style="color:#bf616a;">p</span><span>=embedding_dropout)
</span><span>        </span><span style="color:#bf616a;">self</span><span>.patch_embedding = </span><span style="color:#bf616a;">patch_embed</span><span>(
</span><span>            </span><span style="color:#bf616a;">in_channels</span><span>=in_channels,
</span><span>            </span><span style="color:#bf616a;">patch_size</span><span>=patch_size,
</span><span>            </span><span style="color:#bf616a;">embedding_dim</span><span>=hidden_size
</span><span>        ) 
</span><span>        </span><span style="color:#bf616a;">self</span><span>.vit = nn.</span><span style="color:#bf616a;">Sequential</span><span>(*[</span><span style="color:#bf616a;">encoder</span><span>(
</span><span>            hidden_size, 
</span><span>            num_heads, 
</span><span>            att_dropout, 
</span><span>            batch_first, 
</span><span>            mlp_size, 
</span><span>            mlp_dropout, 
</span><span>            msa, 
</span><span>            mlp)
</span><span>        ] </span><span style="color:#b48ead;">for </span><span style="color:#bf616a;">_ </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(num_layers))
</span><span>        </span><span style="color:#bf616a;">self</span><span>.classifier = nn.</span><span style="color:#bf616a;">Sequential</span><span>(
</span><span>            nn.</span><span style="color:#bf616a;">LayerNorm</span><span>(</span><span style="color:#bf616a;">normalized_shape</span><span>=hidden_size),
</span><span>            nn.</span><span style="color:#bf616a;">Linear</span><span>(</span><span style="color:#bf616a;">in_features</span><span>=hidden_size, </span><span style="color:#bf616a;">out_features</span><span>=num_classes)
</span><span>        )
</span><span>
</span><span>    </span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">X</span><span>):
</span><span>        batch_size = X.shape[</span><span style="color:#d08770;">0</span><span>]
</span><span>        class_token = </span><span style="color:#bf616a;">self</span><span>.class_embedding.</span><span style="color:#bf616a;">expand</span><span>(batch_size, -</span><span style="color:#d08770;">1</span><span>, -</span><span style="color:#d08770;">1</span><span>)
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">patch_embed</span><span>(X)
</span><span>        X = torch.</span><span style="color:#bf616a;">cat</span><span>((class_token, X), </span><span style="color:#bf616a;">dim</span><span>=</span><span style="color:#d08770;">1</span><span>)
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.position_embedding + X
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">embedding_dropout</span><span>(X)
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">transformer_encoder</span><span>(X)
</span><span>        X = </span><span style="color:#bf616a;">self</span><span>.</span><span style="color:#bf616a;">classifier</span><span>(X[:, </span><span style="color:#d08770;">0</span><span>])
</span><span>        </span><span style="color:#b48ead;">return </span><span>X
</span></code></pre>


</div>

    </div>

    <footer>
      <div class="flex flex-col gap-2 mt-16 w-full">
        <div class="flex items-center gap-2.5 mt-1">
          <a href="https://eiasamoah.me/blog/" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted) hover:underline text-[#494949]">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-feather-icon lucide-feather">
              <path d="M12.67 19a2 2 0 0 0 1.416-.588l6.154-6.172a6 6 0 0 0-8.49-8.49L5.586 9.914A2 2 0 0 0 5 11.328V18a1 1 0 0 0 1 1z"/><path d="M16 8 2 22"/><path d="M17.5 15H9"/></svg>
          </a>
          <span>.</span>
          <a href="https://eiasamoah.me/project/" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted) hover:underline text-[#494949]">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-binary-icon lucide-binary">
              <rect x="14" y="14" width="4" height="6" rx="2"/>
              <rect x="6" y="4" width="4" height="6" rx="2"/>
              <path d="M6 20h4"/><path d="M14 10h4"/>
              <path d="M6 14h2v6"/><path d="M14 4h2v6"/>
            </svg>
          </a>
          <span>.</span>
          <a href="https://eiasamoah.me/research/" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted) hover:underline text-[#494949]">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-binoculars-icon lucide-binoculars"><path d="M10 10h4"/><path d="M19 7V4a1 1 0 0 0-1-1h-2a1 1 0 0 0-1 1v3"/><path d="M20 21a2 2 0 0 0 2-2v-3.851c0-1.39-2-2.962-2-4.829V8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v11a2 2 0 0 0 2 2z"/><path d="M 22 16 L 2 16"/><path d="M4 21a2 2 0 0 1-2-2v-3.851c0-1.39 2-2.962 2-4.829V8a1 1 0 0 1 1-1h4a1 1 0 0 1 1 1v11a2 2 0 0 1-2 2z"/>
              <path d="M9 7V4a1 1 0 0 0-1-1H6a1 1 0 0 0-1 1v3"/>
            </svg>
          </a>
          <span>.</span>
          <a href="https://github.com/ikeasamoahansah" target="_blank" rel="noopener noreferrer" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted)">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github">
              <path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path>
              <path d="M9 18c-4.51 2-5-2-7-2"></path>
            </svg>
          </a>
          <span>.</span>
          <a href="https://www.linkedin.com/in/ikeasamoahansah/" target="_blank" rel="noopener noreferrer" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted)">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin">
              <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
              <rect width="4" height="12" x="2" y="9"></rect>
              <circle cx="4" cy="4" r="2"></circle>
            </svg>
          </a>
          <span>.</span>
          <a href="mailto:ikeasamoahansah@outlook.com" class="flex items-center gap-1 transition-colors a-hover text-(--text-muted)">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail">
              <rect width="20" height="16" x="2" y="4" rx="2"></rect>
              <path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path>
          </a>
        </div>
      </div>
    </footer>
  </div>

</body>

</html>